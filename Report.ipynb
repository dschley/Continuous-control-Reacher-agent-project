{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "The Agent implementation can be found in the root directory as the class Agent in the file _agent.py_.\n",
    "\n",
    "The agent can have many instances of itself which all communicate to the same D4PG \"brain\" in order to make decisions and provide its experiences to.  The agent has 2 functions that communicate with the brain: act() which gets the action vector based on the agent's state as in input to the actor network, and step() which simply adds the acted out experience of the agent to the D4PG's memory buffer.\n",
    "\n",
    "The underlying D4PG brain utilized the following hyperparameters for the learning process:\n",
    "#### OUNoise hyperparams\n",
    "- mu = 0\n",
    "- theta = 0.15\n",
    "- sigma = 0.2\n",
    "\n",
    "#### Memory hyperparams\n",
    "- buffer_size = 100000\n",
    "- batch_size = 16\n",
    "\n",
    "#### Update hyperparams\n",
    "- gamma = 0.99\n",
    "- tau = 0.01\n",
    "- alr = 1e-4\n",
    "- clr = 1e-3\n",
    "\n",
    "The learning algorithm works as follows:\n",
    "1. Each agent gets the action vector that it should perform from the local actor network giving the agent's state as input.  Some Ornstein Uhlenbeck noise is also added to the action vector in order to introduce some small level of exploration but not in an erratic but more of a consistent and predictable amount of random noise.\n",
    "2. Every agent takes a step in the environment using the action vector from step 1 and observes the reward, next state, and done status and feeds all of this information in to one memory buffer shared by all agents.\n",
    "3. After every time step a small batch from the buffer is used for learning for the actor and critic networks\n",
    "4. A) The Actor learns by utilizing the critic's opinion of how the actor chose an action from the state.  More formally, it takes a step in the direction of maximizing Critic(state, Actor(state)) where Critic gives the value of the state and action, the action here being the Actor's decision of what the action should be based on the current state.\n",
    "4. B) The Critic learns by comparing its value evaluation of the current state and action to the observed reward and discounted(gamma) value of the next state and the actions that the actor would take from that state under the current policy.  Both of these learning steps appropriately use target networks so that learning is more stable and updates these target networks based on the hyperparameter tau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic models\n",
    "\n",
    "#### Actor\n",
    "The actor model is found as the class Actor in the file agent.py.  The model consists of 3 hidden layers: \n",
    "- fully connected, size 32, relu activation\n",
    "- fully connected, size 64, relu activation\n",
    "- fully connected, size 64, relu activation\n",
    "\n",
    "The output layer has a tanh activation function in order to bound the action vector's values between -1 and 1.\n",
    "\n",
    "#### Critic\n",
    "The critic model is found as the class Critic in the file agent.py.  The model combines the state and action vector as one tensor and uses that as the input.  There are 2 hidden layers in this network:\n",
    "- fully connected, size 64, relu activation\n",
    "- fully connected, size 64, relu activation\n",
    "\n",
    "The output layer uses the natural output in order to give the normal, unbounded value of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards plot\n",
    "Below are the plots of the score progressions for both the individual agents as well as their average scores over time.  I was discouraged at first with the slow climb in the beginning but decided to take a break and let it just run and to my surprise it had a huge jump in score at about episode 300 (possibly one agent taking an outlandish exploration that actually worked and they all learned from that experience).  Upon reaching the 430th episode, the previous 100 episodes had and average score, over the average scores of each agent, of over 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"individual_training_scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"avg_training_scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Future Improvements\n",
    "One possible improvement that I actually started implementing before I notice the jump in performance was to speed up the process by using a single \"multi-agent\" rather than multiple agents and processing each state through the network individually.  Instead it would just put all of the states in one tensor, evaluate that, and apply those actions to the environment.  \n",
    "\n",
    "Additionally, some clipping out of the Critic network, which is previously unbounded, could improve the learning by preventing the exploding gradients problem.\n",
    "\n",
    "It would be impossible for this environment due to all of the agents having to interact with one environment at once, but introducing more distribution by actually multithreading, multiprocessing, or some other form of distributed computing.  The individual agents could also be configured to convene back to the central \"brain\" after a few more time steps and utilize these extra steps to do some n-step bootstrapping to add to the critic's learning regiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
